{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun With Word2Vec: News Headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will:\n",
    "* Clean text data (news article titles and headlines from [this paper](https://www.researchgate.net/publication/322652391_Multi-Source_Social_Feedback_of_Online_News_Feeds))\n",
    "* Use Word2Vec to create word embeddings, and visualize word clusters on a t-SNE plot\n",
    "* Do the same with titles, by averaging the word vectors in each title\n",
    "* Train a model based on title embedding, topic, time since publishing, and sentiment, in order to predict the article's popularity on each platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T02:53:57.504806Z",
     "start_time": "2020-07-06T02:53:50.589713Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:00:24.640752Z",
     "start_time": "2020-07-06T03:00:24.619823Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:00:25.630808Z",
     "start_time": "2020-07-06T03:00:25.609101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set options for pandas\n",
    "\n",
    "def start():\n",
    "    options = {\n",
    "        'display': {\n",
    "            'max_columns': None,\n",
    "            'max_colwidth': 25,\n",
    "            'expand_frame_repr': False,  # Don't wrap to multiple pages\n",
    "            'max_rows': 14,\n",
    "            'max_seq_items': 50,         # Max length of printed sequence\n",
    "            'precision': 4,\n",
    "            'show_dimensions': False\n",
    "        },\n",
    "        'mode': {\n",
    "            'chained_assignment': None   # Controls SettingWithCopyWarning\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for category, option in options.items():\n",
    "        for op, value in option.items():\n",
    "            pd.set_option(f'{category}.{op}', value)  # Python 3.6+\n",
    "\n",
    "start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:00:27.847833Z",
     "start_time": "2020-07-06T03:00:27.082390Z"
    }
   },
   "outputs": [],
   "source": [
    "main_data = pd.read_csv('News_Final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:00:28.345299Z",
     "start_time": "2020-07-06T03:00:28.246114Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We'll start by doing some cleaning and manipulation on the text data. In particular, we need to:\n",
    "1. Get individual words from article titles\n",
    "2. Use NLTK to filter out stopwords, punctuation, and any words that are not known to Word2Vec\n",
    "3. Turn the words into vector representations using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:00:30.849622Z",
     "start_time": "2020-07-06T03:00:30.830669Z"
    }
   },
   "outputs": [],
   "source": [
    "# Grab all the article titles\n",
    "article_titles = main_data['Title']\n",
    "article_titles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:00:38.584959Z",
     "start_time": "2020-07-06T03:00:31.602444Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of strings, where each string is an article title\n",
    "titles_list = [title for title in article_titles]\n",
    "\n",
    "# Collapse the list of strings into a single long string for processing\n",
    "big_title_string = ' '.join(titles_list)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the string into words\n",
    "tokens = word_tokenize(big_title_string)\n",
    "\n",
    "# Remove non-alphabetic tokens, such as punctuation\n",
    "words = [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "# Filter out stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "words = [word for word in words if not word in stop_words]\n",
    "\n",
    "# Print first 10 words\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:03:54.810708Z",
     "start_time": "2020-07-06T03:01:30.691116Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Word2Vec model (trained on an enormous Google corpus)\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True) \n",
    "\n",
    "# Check dimension of word vectors\n",
    "model.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a word vector looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:04:02.456609Z",
     "start_time": "2020-07-06T03:04:02.423925Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "economy_vec = model['economy']\n",
    "economy_vec[:20] # First 20 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:04:20.441536Z",
     "start_time": "2020-07-06T03:04:04.074781Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter the list of vectors to include only those that Word2Vec has a vector for\n",
    "vector_list = [model[word] for word in words if word in model.vocab]\n",
    "\n",
    "# Create a list of the words corresponding to these vectors\n",
    "words_filtered = [word for word in words if word in model.vocab]\n",
    "\n",
    "# Zip the words together with their vector representations\n",
    "word_vec_zip = zip(words_filtered, vector_list)\n",
    "\n",
    "# Cast to a dict so we can turn it into a dataframe\n",
    "word_vec_dict = dict(word_vec_zip)\n",
    "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a bunch of tidy vectors in a DataFrame. Each column is the word's component for that dimension (300 dimensions total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:04:26.653928Z",
     "start_time": "2020-07-06T03:04:26.241454Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction: t-SNE\n",
    "\n",
    "Next, we're going to squish (read: do dimensionality reduction on) these word vectors using t-SNE. If you're not familiar with t-SNE and its interpretations, check out [this excellent, interactive distill.pub article](https://distill.pub/2016/misread-tsne/) on t-SNE. \n",
    "\n",
    "It's important to play around with the parameters for t-SNE, as different values can produce very different results. I tested several values between 0 and 100 for perplexity, and found that it produced roughly the same shape each time. I tested several learning rates between 20 and 400 as well, and decided to leave the learning rate at its default (200)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:04:43.767607Z",
     "start_time": "2020-07-06T03:04:35.939538Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
    "\n",
    "# Use only 400 rows to shorten processing time\n",
    "tsne_df = tsne.fit_transform(df[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to plot our reduced array of word vectors and see what patterns we can find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:06:29.380297Z",
     "start_time": "2020-07-06T03:05:57.584954Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
    "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
    "\n",
    "# Second plot: Import adjustText, initialize list of texts\n",
    "from adjustText import adjust_text\n",
    "texts = []\n",
    "words_to_plot = list(np.arange(0, 400, 10))\n",
    "\n",
    "# Append words to list\n",
    "for word in words_to_plot:\n",
    "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
    "    \n",
    "# Plot text using adjust_text (because overlapping text is hard to read)\n",
    "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
    "            expand_points = (2,1), expand_text = (1,2),\n",
    "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even though the text still overlaps some points, we no longer have overlapping text. Nifty! \n",
    "\n",
    "If you're interested in trying out `adjust_text` for your own plotting needs, you can find it [here](https://github.com/Phlya/adjustText). Be sure to import using the camelcase `adjustText`, and please note that `adjustText` is currently not compatible with matplotlib 3.0 or higher.\n",
    "\n",
    "It's encouraging to see that even when the vector embeddings have been reduced to 2 dimensions, we see certain items clustering together. For example, we have **months** in the left/upper left, we have **corporate finance terms** near the bottom, and we have more **generic, non-topical words** (like 'full', 'really', 'slew') in the middle.\n",
    "\n",
    "Note that if we were to run the t-SNE again with different parameters, we may observe some similarities to this result, but we're not guaranteed to see the exact same patterns. t-SNE, as compared to PCA, is not deterministic. Relatedly, tightness of clusters and distances between clusters are not always meaningful. It is meant primarily as an exploratory tool, rather than as a decisive indicator of similarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Averaging Word Embeddings \n",
    "\n",
    "We've gotten a sense of how word embeddings work as applied to this dataset. Now we can move on to some more interesting ML applications: finding titles that cluster together, and seeing what patterns emerge.\n",
    "\n",
    "Instead of using Doc2Vec, which would require a lengthy training process, we can use a simpler [(and sometimes even more effective)](https://stackoverflow.com/questions/45234310/doc2vec-worse-than-mean-or-sum-of-word2vec-vectors) trick: averaging the embeddings of the word vectors in each document. In our case, a document refers to a title or a headline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:09:08.295493Z",
     "start_time": "2020-07-06T03:09:08.246053Z"
    }
   },
   "outputs": [],
   "source": [
    "def document_vector(word2vec_model, doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in model.vocab]\n",
    "    return np.mean(model[doc], axis=0)\n",
    "\n",
    "# Our earlier preprocessing was done when we were dealing only with word vectors\n",
    "# Here, we need each document to remain a document \n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    doc = word_tokenize(text)\n",
    "    doc = [word for word in doc if word not in stop_words]\n",
    "    doc = [word for word in doc if word.isalpha()] \n",
    "    return doc\n",
    "\n",
    "# Function that will help us drop documents that have no word vectors in word2vec\n",
    "def has_vector_representation(word2vec_model, doc):\n",
    "    \"\"\"check if at least one word of the document is in the\n",
    "    word2vec dictionary\"\"\"\n",
    "    return not all(word not in word2vec_model.vocab for word in doc)\n",
    "\n",
    "# Filter out documents\n",
    "def filter_docs(corpus, texts, condition_on_doc):\n",
    "    \"\"\"\n",
    "    Filter corpus and texts given the function condition_on_doc which takes\n",
    "    a doc. The document doc is kept if condition_on_doc(doc) is true.\n",
    "    \"\"\"\n",
    "    number_of_docs = len(corpus)\n",
    "\n",
    "    if texts is not None:\n",
    "        texts = [text for (text, doc) in zip(texts, corpus)\n",
    "                 if condition_on_doc(doc)]\n",
    "\n",
    "    corpus = [doc for doc in corpus if condition_on_doc(doc)]\n",
    "\n",
    "    print(\"{} docs removed\".format(number_of_docs - len(corpus)))\n",
    "\n",
    "    return (corpus, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:09:28.839015Z",
     "start_time": "2020-07-06T03:09:12.561061Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess the corpus\n",
    "corpus = [preprocess(title) for title in titles_list]\n",
    "\n",
    "# Remove docs that don't include any words in W2V's vocab\n",
    "corpus, titles_list = filter_docs(corpus, titles_list, lambda doc: has_vector_representation(model, doc))\n",
    "\n",
    "# Filter out any empty docs\n",
    "corpus, titles_list = filter_docs(corpus, titles_list, lambda doc: (len(doc) != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:09:50.810834Z",
     "start_time": "2020-07-06T03:09:41.516653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize an array for the size of the corpus\n",
    "x = []\n",
    "for doc in corpus: # append the vector for each document\n",
    "    x.append(document_vector(model, doc))\n",
    "    \n",
    "X = np.array(x) # list to array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedure and functions adapted from [this notebook](https://github.com/sdimi/average-word2vec/blob/master/notebook.ipynb) by Dimitris Spathis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE for Document Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:10:19.435330Z",
     "start_time": "2020-07-06T03:10:13.208067Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
    "\n",
    "# Use only 400 rows to shorten processing time\n",
    "tsne_df = tsne.fit_transform(X[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:10:49.026010Z",
     "start_time": "2020-07-06T03:10:41.850626Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (14, 10))\n",
    "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
    "\n",
    "# Second plot: Import adjustText, initialize list of texts\n",
    "from adjustText import adjust_text\n",
    "texts = []\n",
    "titles_to_plot = list(np.arange(0, 400, 40)) # plots every 40th title in first 400 titles\n",
    "\n",
    "# Append words to list\n",
    "for title in titles_to_plot:\n",
    "    texts.append(plt.text(tsne_df[title, 0], tsne_df[title, 1], titles_list[title], fontsize = 14))\n",
    "    \n",
    "# Plot text using adjust_text (because overlapping text is hard to read)\n",
    "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
    "            expand_points = (2,1), expand_text = (1,2),\n",
    "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty interesting! We can see that the t-SNE has collapsed the document vectors into a dimensional space where the documents are spread out based on whether their content has more to do with countries, world leaders, and foreign affairs, or has more to do with technology companies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:11:30.788492Z",
     "start_time": "2020-07-06T03:11:30.405760Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop all the rows where the article popularities are unknown /(this is only about 11% of the data)\n",
    "main_data = main_data.drop(main_data[(main_data.Facebook == -1) | \n",
    "                                     (main_data.GooglePlus == -1) | \n",
    "                                     (main_data.LinkedIn == -1)].index)\n",
    "\n",
    "# Also drop all rows where we don't know the source\n",
    "main_data = main_data.drop(main_data[main_data['Source'].isna()].index)\n",
    "\n",
    "main_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popularity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have 81,000 rows left to work with. \n",
    "\n",
    "Let's see if we can find any associations between sentiment and number of shares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:11:36.847983Z",
     "start_time": "2020-07-06T03:11:34.134726Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 10))\n",
    "\n",
    "subplots = [a for a in ax]\n",
    "platforms = ['Facebook', 'GooglePlus', 'LinkedIn']\n",
    "colors = list(sns.husl_palette(10, h=.5)[1:4]) \n",
    "\n",
    "for platform, subplot, color in zip(platforms, subplots, colors):\n",
    "    sns.scatterplot(x = main_data[platform], y = main_data['SentimentTitle'], ax=subplot, color=color)\n",
    "    subplot.set_title(platform, fontsize=18)\n",
    "    subplot.set_xlabel('') \n",
    "    \n",
    "fig.suptitle('Plot of Popularity (Shares) by Title Sentiment', fontsize=24)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit hard to make out whether there's any relationship here, since a few articles are significant outliers in terms of their share counts. Let's try log-transforming the x-axis to see if we can reveal any patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:11:52.351282Z",
     "start_time": "2020-07-06T03:11:49.460924Z"
    }
   },
   "outputs": [],
   "source": [
    "# Our data has over 80,000 rows, so let's subsample it to make the log-transformed scatterplot easier to read\n",
    "\n",
    "subsample = main_data.sample(5000)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 10))\n",
    "\n",
    "subplots = [a for a in ax]\n",
    "\n",
    "for platform, subplot, color in zip(platforms, subplots, colors):\n",
    "    # Regression plot, so we can gauge the linear relationship\n",
    "    sns.regplot(x = np.log(subsample[platform] + 1), y = subsample['SentimentTitle'], \n",
    "                ax=subplot, \n",
    "                color=color,\n",
    "                # Pass an alpha value to regplot's scatterplot call\n",
    "                scatter_kws={'alpha':0.5})\n",
    "    \n",
    "    # Set a nice title, get rid of x labels\n",
    "    subplot.set_title(platform, fontsize=18)\n",
    "    subplot.set_xlabel('') \n",
    "    \n",
    "fig.suptitle('Plot of log(Popularity) by Title Sentiment', fontsize=24)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to what we might expect (from our idea of highly emotional, clickbaity headlines), in this dataset we find no relationship between headline sentiment and article popularity as measured by number of shares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:11:56.833099Z",
     "start_time": "2020-07-06T03:11:56.707693Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Convert publish date column to make it compatible with other datetime objects\n",
    "\n",
    "main_data['PublishDate'] = pd.to_datetime(main_data['PublishDate'])\n",
    "\n",
    "# Time since Linux Epoch\n",
    "t = datetime.datetime(1970, 1, 1)\n",
    "\n",
    "# Subtract this time from each article's publish date\n",
    "main_data['TimeSinceEpoch'] = main_data['PublishDate'] - t\n",
    "\n",
    "# Create another column for just the days from the timedelta objects \n",
    "main_data['DaysSinceEpoch'] = main_data['TimeSinceEpoch'].astype('timedelta64[D]')\n",
    "\n",
    "main_data['TimeSinceEpoch'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like all of the articles were published within about a 200-day window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:12:04.793730Z",
     "start_time": "2020-07-06T03:12:00.785704Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the list of top 12 sources by number of articles\n",
    "source_names = list(main_data['Source'].value_counts()[:12].index)\n",
    "source_colors = list(sns.husl_palette(12, h=.5))\n",
    "\n",
    "fig, ax = plt.subplots(4, 3, figsize=(20, 15), sharex=True, sharey=True)\n",
    "\n",
    "ax = ax.flatten()\n",
    "for ax, source, color in zip(ax, source_names, source_colors):\n",
    "    sns.distplot(main_data.loc[main_data['Source'] == source]['SentimentTitle'],\n",
    "                               ax=ax, color=color, kde_kws={'shade':True})\n",
    "    ax.set_title(source, fontsize=14)\n",
    "    ax.set_xlabel('')\n",
    "    \n",
    "plt.xlim(-0.75, 0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:12:12.226498Z",
     "start_time": "2020-07-06T03:12:11.497575Z"
    }
   },
   "outputs": [],
   "source": [
    "# Overlay each density curve on the same plot for closer comparison\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "for source, color in zip(source_names, source_colors):\n",
    "    sns.distplot(main_data.loc[main_data['Source'] == source]['SentimentTitle'],\n",
    "                               ax=ax, hist=False, label=source, color=color)\n",
    "    ax.set_xlabel('')\n",
    "    \n",
    "plt.xlim(-0.75, 0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By overlaying the density estimates onto the same plot, we can get a better idea of how their shapes differ than trying to compare each plot to the other. \n",
    "\n",
    "We see that the sources' Sentiment distributions (for article titles) are very similar - it doesn't look like any one source is an outlier in terms of positive or negative headlines. Instead, all 12 of the most common sources have distributions centered around 0 with modestly sized tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:12:17.584841Z",
     "start_time": "2020-07-06T03:12:16.124589Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(15, 10))\n",
    "\n",
    "subplots = [a for a in ax]\n",
    "\n",
    "for platform, subplot, color in zip(platforms, subplots, colors):\n",
    "  \n",
    "    sns.distplot(np.log(main_data[platform] + 1), ax=subplot, color=color, kde_kws={'shade':True})\n",
    "    \n",
    "    # Set a nice title, get rid of x labels\n",
    "    subplot.set_title(platform, fontsize=18)\n",
    "    subplot.set_xlabel('') \n",
    "    \n",
    "fig.suptitle('Plot of Popularity by Platform', fontsize=24)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popularity Prediction\n",
    "\n",
    "Our exploratory visualizations may not have yielded much to work from, but we can still see if there are any non-visual patterns we've missed that a model might discover. Our first task in preparing the data for modeling is to rejoin the document vectors with their respective titles. Thankfully, when we were preprocessing the corpus, we processed the `corpus` and `titles_list` simultaneously, so the vectors and the titles they represent will still match up. Meanwhile, in `main_df`, we have dropped all of the articles that had -1 popularity, so we'll need to drop the vectors that represent those article titles.\n",
    "\n",
    "Training a model on these enormous vectors as-is will not be possible on this computer, but we'll see what we can do with a little dimension reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:14:29.991679Z",
     "start_time": "2020-07-06T03:14:25.756003Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=15, random_state=10)\n",
    "\n",
    "# x is the array with our 300-dimensional vectors\n",
    "reduced_vecs = pca.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:14:33.449665Z",
     "start_time": "2020-07-06T03:14:32.962588Z"
    }
   },
   "outputs": [],
   "source": [
    "df_w_vectors = pd.DataFrame(reduced_vecs)\n",
    "\n",
    "df_w_vectors['Title'] = titles_list\n",
    "\n",
    "main_w_vectors = pd.concat((df_w_vectors, main_data), axis=1)\n",
    "\n",
    "# Get rid of vectors that couldn't be matched with the main_df\n",
    "main_w_vectors.dropna(axis=0, inplace=True)\n",
    "\n",
    "main_w_vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:14:41.163276Z",
     "start_time": "2020-07-06T03:14:41.148816Z"
    }
   },
   "outputs": [],
   "source": [
    "# All of the columns have been successfully appended \n",
    "list(main_w_vectors.columns)[-13:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:14:43.593618Z",
     "start_time": "2020-07-06T03:14:43.357568Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop all non-numeric, non-dummy columns, for feeding into the models\n",
    "cols_to_drop = ['IDLink', 'Title', 'TimeSinceEpoch', 'Headline', 'PublishDate', 'Source'] \n",
    "\n",
    "data_only_df = pd.get_dummies(main_w_vectors, columns = ['Topic']).drop(columns=cols_to_drop)\n",
    "\n",
    "# Standardize DaysSinceEpoch since the raw numbers are larger in magnitude \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshape so we can feed the column to the scaler\n",
    "standardized_days = np.array(data_only_df['DaysSinceEpoch']).reshape(-1, 1)\n",
    "data_only_df['StandardizedDays'] = scaler.fit_transform(standardized_days)\n",
    "\n",
    "# Drop the raw column; we don't need it anymore\n",
    "data_only_df.drop(columns=['DaysSinceEpoch'], inplace=True)\n",
    "\n",
    "# Look at the new range\n",
    "data_only_df['StandardizedDays'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:14:48.486628Z",
     "start_time": "2020-07-06T03:14:48.435720Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create individual DataFrames for each platform's regression task\n",
    "fb_data_only_df = data_only_df.drop(columns=['GooglePlus', 'LinkedIn'])\n",
    "goog_data_only_df = data_only_df.drop(columns=['Facebook', 'LinkedIn'])\n",
    "li_data_only_df = data_only_df.drop(columns=['Facebook', 'GooglePlus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Popularity: First Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:14:50.010608Z",
     "start_time": "2020-07-06T03:14:49.769024Z"
    }
   },
   "outputs": [],
   "source": [
    "X = fb_data_only_df.drop('Facebook', axis=1)\n",
    "y = fb_data_only_df['Facebook']\n",
    "\n",
    "# 80% of data goes to training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Instantiate an XGBRegressor\n",
    "xgr = xgb.XGBRegressor(random_state=2)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgr.predict(X_test)\n",
    "\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underwhelming results - let's do some hyperparameter tuning and see if we can improve. Hyperparameter grid repurposed from [this Kaggle article.](https://www.kaggle.com/jayatou/xgbregressor-with-gridsearchcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Various hyper-parameters to tune\n",
    "xgb1 = xgb.XGBRegressor()\n",
    "parameters = {'nthread':[4], \n",
    "              'objective':['reg:linear'],\n",
    "              'learning_rate': [.03, 0.05, .07], \n",
    "              'max_depth': [5, 6, 7],\n",
    "              'min_child_weight': [4],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.7],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [250]}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb1,\n",
    "                        parameters,\n",
    "                        cv = 2,\n",
    "                        n_jobs = 5,\n",
    "                        verbose=True)\n",
    "\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 5, 'min_child_weight': 4, \n",
    "          'n_estimators': 250, 'nthread': 4, 'objective': 'reg:linear', 'silent': 1, 'subsample': 0.7}\n",
    "\n",
    "# Try again with new params\n",
    "xgr = xgb.XGBRegressor(random_state=2, **params)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgr.predict(X_test)\n",
    "\n",
    "mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Well, it -is- better than the base model by about 30,000, but still not ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the data in its current state seems insufficient for this model to perform. Let's see if we can improve it with a little feature engineering: we'll train some classifiers to separate the two main groups of articles: Duds (0 or 1 share) vs. Not Duds. \n",
    "\n",
    "The idea is that if we can give the regressor a feature (the probability that the article will have extremely low shares), it may perform more favorably on predicting the more highly-shared articles, thus lowering the residual values for those articles and reducing mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: Detect Dud Articles\n",
    "\n",
    "Let's take a look at the distribution of FB popularity as a whole:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fb_data_only_df['Facebook'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that 25% of articles don't even get one share. Also, from the log-transformed plots we made earlier, we can note that in general, there are 2 chunks of articles: 1 cluster at 0, and another cluster (the long tail) going from 1 onwards. We can train a few classifiers to identify whether the article will be a \"dud\" (be in the 0 shares bin), and then use the predictions of those models as features for the final regressor, which will predict probability. This is called **model stacking**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a quick function that will return 1 (true) if the article has 0-1 share(s)\n",
    "def dud_finder(popularity):\n",
    "    if popularity <= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Create target column using the function\n",
    "fb_data_only_df['is_dud'] = fb_data_only_df['Facebook'].apply(dud_finder)\n",
    "fb_data_only_df[['Facebook', 'is_dud']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_data_only_df['is_dud'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28% of articles can be classified as \"duds\"\n",
    "fb_data_only_df['is_dud'].sum() / len(fb_data_only_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dud feature made, we'll initialize the classifiers. We'll use a Random Forest, an optimized XGBClassifier, and a K-Nearest Neighbors classifier. First, the XGB, since it requires a lot of fiddling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fb_data_only_df.drop('is_dud', axis=1)\n",
    "y = fb_data_only_df['is_dud']\n",
    "\n",
    "# 80% of data goes to training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters to tune\n",
    "xgc = xgb.XGBClassifier()\n",
    "parameters = {'nthread':[4],\n",
    "              'learning_rate': [.03, 0.05, .07], \n",
    "              'max_depth': [5, 6, 7],\n",
    "              'min_child_weight': [4],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.7],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [100]}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgc,\n",
    "                        parameters,\n",
    "                        cv = 2,\n",
    "                        n_jobs = 5,\n",
    "                        verbose=True)\n",
    "\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give it 200 estimators, 100 is just a restriction to make the grid search run faster\n",
    "params = {'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 5, 'min_child_weight': 4, \n",
    "          'n_estimators': 200, 'nthread': 4, 'silent': 1, 'subsample': 0.7}\n",
    "\n",
    "# Try again with new params\n",
    "xgc = xgb.XGBClassifier(random_state=10, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = fb_data_only_df.drop(['is_dud', 'Facebook'], axis=1)\n",
    "y = fb_data_only_df['is_dud']\n",
    "\n",
    "# 70% of data goes to training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 10)\n",
    "\n",
    "#lr = LogisticRegression(solver='liblinear', multi_class='auto', random_state=10)\n",
    "#xgc = xgb.XGBClassifier(random_state=10)\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=10)\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "preds = {}\n",
    "for model_name, model in zip(['XGClassifier', 'RandomForestClassifier', 'KNearestNeighbors'], [xgc, rfc, knn]):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds[model_name] = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score\n",
    "\n",
    "for k in preds:\n",
    "    print(\"{} performance:\".format(k))\n",
    "    print()\n",
    "    print(classification_report(y_test, preds[k]), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "for model in [xgc, rfc, knn]:\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr)\n",
    "    \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Popularity: Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can add the averaged probabilities that each article is a dud to help out the regressor\n",
    "averaged_probs = (xgc.predict_proba(X)[:, 1] +\n",
    "                  knn.predict_proba(X)[:, 1] + \n",
    "                  rfc.predict_proba(X)[:, 1]) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['prob_dud'] = averaged_probs\n",
    "y = fb_data_only_df['Facebook']\n",
    "\n",
    "# Reinitialize the train_test_split since we've changed back to regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 10)\n",
    "\n",
    "# Another round of hyperparameter tuning\n",
    "xgb1 = xgb.XGBRegressor()\n",
    "parameters = {'nthread':[4], \n",
    "              'objective':['reg:linear'],\n",
    "              'learning_rate': [.03, .05, .07], \n",
    "              'max_depth': [5, 6, 7],\n",
    "              'min_child_weight': [4],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.7],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [250]}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb1,\n",
    "                        parameters,\n",
    "                        cv = 2,\n",
    "                        n_jobs = 5,\n",
    "                        verbose=True)\n",
    "\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 5, 'min_child_weight': 4, \n",
    "          'n_estimators': 250, 'nthread': 4, 'objective': 'reg:linear', 'silent': 1, 'subsample': 0.7}\n",
    "\n",
    "# Try again with new params\n",
    "xgr = xgb.XGBRegressor(random_state=2, **params)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgr.predict(X_test)\n",
    "\n",
    "mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Oops...the performance is essentially the same as it was before any model stacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature, importance in zip(list(X.columns), xgr.feature_importances_):\n",
    "    print('Model weight for feature {}: {}'.format(feature, importance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can at least claim one victory: the model thought that `prob_dud` was the most important feature! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
